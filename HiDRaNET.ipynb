{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaPareti/ML_course_Pavia_23/blob/main/HiDRaNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99L6z4K8HRDB"
      },
      "source": [
        "A few imports that will be used in the following"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRTRcMb8HPjq"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXdKP9h0Q5uJ"
      },
      "source": [
        "Install Pytorch Geometric libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fix0upCibG2C"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E4zMo9jQ4t6"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch_cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1waWV7HtD_UA"
      },
      "source": [
        "Load Dataset: 200 events per energy set, with primary particle energy in [10, 20, 30, 40, 60, 80, 100] GeV.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M23mDSUyD0mA"
      },
      "outputs": [],
      "source": [
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/7GrigA2KQ7uDvR4/hits_piplus_10GeV.csv -o hits_piplus_10GeV.csv\n",
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/wPraydaZYzRL18I/hits_piplus_20GeV.csv -o hits_piplus_20GeV.csv\n",
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/Sckke3sSgbJnZPT/hits_piplus_30GeV.csv -o hits_piplus_30GeV.csv\n",
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/eGtiQK5ciKXyUTu/hits_piplus_40GeV.csv -o hits_piplus_40GeV.csv\n",
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/l6o77MgGgdD0tbr/hits_piplus_60GeV.csv -o hits_piplus_60GeV.csv\n",
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/TnevGLZ0Z1GawlT/hits_piplus_80GeV.csv -o hits_piplus_80GeV.csv\n",
        "!curl https://cernbox.cern.ch/remote.php/dav/public-files/mPuwiJldxiIvibi/hits_piplus_100GeV.csv -o hits_piplus_100GeV.csv\n",
        "\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5YkPdr5D7tW"
      },
      "source": [
        "Load Dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGTS2yXlHEx-",
        "outputId": "8e03c49c-b66d-4b4a-866b-cb2302c17e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.0.1+cu118\n",
            "CUDA GPU: False\n",
            "device :  cpu\n"
          ]
        }
      ],
      "source": [
        "def print_device_usage(device):\n",
        "    tot_memory = torch.cuda.get_device_properties(device).total_memory/1024.0**3\n",
        "    reserved_memory = torch.cuda.memory_reserved(device)/1024.0**3\n",
        "    allocated_memory = torch.cuda.memory_allocated(device)/1024.0**3\n",
        "    free_memory = reserved_memory-allocated_memory  # free inside reserved\n",
        "    print('Total memory in Gb: %.2f'%tot_memory)\n",
        "    print('Reserved memory in Gb: %.2f'%reserved_memory)\n",
        "    print('Allocated memory in Gb: %.2f'%allocated_memory)\n",
        "    print('Free memory in Gb: %.2f'%free_memory)\n",
        "\n",
        "print(torch.__version__)\n",
        "print(\"CUDA GPU:\", torch.cuda.is_available())\n",
        "torch.manual_seed(42)\n",
        "# check if a GPU is available. Otherwise run on CPU\n",
        "device = 'cpu'\n",
        "args_cuda = torch.cuda.is_available()\n",
        "if args_cuda: device = \"cuda:0\"\n",
        "print('device : ',device)\n",
        "if args_cuda: print_device_usage(device)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S2jrfxpKqVo"
      },
      "source": [
        "Define features of each graph and of nodes (hits in the calirimeter)\n",
        "\n",
        "And load the data as Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b3ryxHCHM1g"
      },
      "outputs": [],
      "source": [
        "input_features = ['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord']\n",
        "node_features = ['PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'x', 'y', 'z', 'E']\n",
        "\n",
        "myDF10 = pd.read_csv('hits_piplus_10GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "myDF20 = pd.read_csv('hits_piplus_20GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "myDF30 = pd.read_csv('hits_piplus_30GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "myDF40 = pd.read_csv('hits_piplus_40GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "myDF60 = pd.read_csv('hits_piplus_60GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "myDF80 = pd.read_csv('hits_piplus_80GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "myDF100 = pd.read_csv('hits_piplus_100GeV.csv', sep=\"\\t\", names=['evtID', 'truthE', 'truthPDG','PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'hitsCoord'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdfCwjLdQZRR"
      },
      "source": [
        "Import libraries for Graph NNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NEiWxlHQX7r"
      },
      "outputs": [],
      "source": [
        "import torch_geometric\n",
        "from torch_geometric.nn import knn_graph, EdgeConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
        "from torch_cluster import knn_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjiDA8FIK2Ff"
      },
      "source": [
        "Import Data as dataframe, with every row consisting of one event\n",
        "\n",
        "The last column is a list of size \"n_nodes\", and each element is another list of 4 feautures: (X,Y,Z) spatial coordinates and the energy deposit in the hit E (each event has a different number of hits in the calorimeter, and hence, different number of nodes)\n",
        "\n",
        "Some cleanup is required to pass through this dataset to a GNN: first of all, for each event (graph) the graph-level features are extended to each node.\n",
        "\n",
        "\n",
        "Let's define a function to clear up this dataset:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDAdjwAHL1rn"
      },
      "outputs": [],
      "source": [
        "from ast import literal_eval\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "def make_dataset(dataset, df, hits_column_name, target_name, input_features, node_features):\n",
        "    print(df)\n",
        "    df[hits_column_name] = df[hits_column_name].fillna(\"[]\").apply(lambda x: eval(x))\n",
        "    target = np.array(df[target_name])\n",
        "    n_events = df[hits_column_name].shape[0]\n",
        "    print(\"Number of events (Graphs): \", n_events)\n",
        "    for event in range(n_events):\n",
        "        # If the number of energy deposits is larger than 1000, filter only the 1000 larger ones\n",
        "        n_nodes = min(np.array(df[hits_column_name][event]).shape[0], 1000)\n",
        "        x = np.array([   item[0] for item in  np.array(df[hits_column_name][event])   ])\n",
        "        y = np.array([   item[1] for item in  np.array(df[hits_column_name][event])   ])\n",
        "        z = np.array([   item[2] for item in  np.array(df[hits_column_name][event])   ])\n",
        "        E = np.array([   item[3] for item in  np.array(df[hits_column_name][event])   ])\n",
        "        truthE = np.full(x.shape, df.truthE[event])\n",
        "        PMTenergyS = np.full(x.shape, df.PMTenergyS[event])\n",
        "        PMTenergyC = np.full(x.shape, df.PMTenergyC[event])\n",
        "        PMTenergyComb = np.full(x.shape, df.PMTenergyComb[event])\n",
        "        SiPMenergyS = np.full(x.shape, df.SiPMenergyS[event])\n",
        "        SiPMenergyC = np.full(x.shape, df.SiPMenergyC[event])\n",
        "        SiPMenergyComb = np.full(x.shape, df.SiPMenergyComb[event])\n",
        "        hit_data = np.stack((PMTenergyS, PMTenergyC, PMTenergyComb, SiPMenergyS, SiPMenergyC, SiPMenergyComb, x, y, z, E), axis=-1)\n",
        "        eventDF = pd.DataFrame(hit_data, columns=['PMTenergyS', 'PMTenergyC', 'PMTenergyComb', 'SiPMenergyS', 'SiPMenergyC', 'SiPMenergyComb', 'x', 'y', 'z', 'E']).nlargest(n_nodes, 'E')\n",
        "        d = Data(x = torch.tensor(eventDF.values, dtype=torch.float), y = torch.tensor(truthE[event], dtype=torch.float), num_nodes = n_nodes)\n",
        "        dataset.append(d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-z4B21gP0Gw"
      },
      "source": [
        "Now append all graphs to a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d58IFzcZPzqs"
      },
      "outputs": [],
      "source": [
        "hit_dataset = []\n",
        "make_dataset(hit_dataset, myDF10, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "make_dataset(hit_dataset, myDF20, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "make_dataset(hit_dataset, myDF30, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "make_dataset(hit_dataset, myDF40, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "make_dataset(hit_dataset, myDF60, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "make_dataset(hit_dataset, myDF80, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "make_dataset(hit_dataset, myDF100, 'hitsCoord', 'truthE', input_features, node_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyS072L1bbS6"
      },
      "source": [
        "We can now proceed with splitting the whole dataset in a training and a validation sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWAx48oYbj9K",
        "outputId": "f5ccc563-99ff-4904-ac03-2793f49c7326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataBatch(x=[28081, 10], y=[32], num_nodes=28081, batch=[28081], ptr=[33])\n",
            "tensor([ 0,  0,  0,  ..., 31, 31, 31])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset_size = len(hit_dataset)\n",
        "val_split = 0.2\n",
        "val_size = int(val_split * dataset_size)\n",
        "train_size = dataset_size - val_size\n",
        "hit_dataset_train, hit_dataset_val = train_test_split(hit_dataset, test_size = val_size, train_size = train_size, shuffle=True)\n",
        "\n",
        "batch_size = 32\n",
        "hit_loader_train = DataLoader(hit_dataset_train, batch_size=batch_size, shuffle=True)\n",
        "hit_loader_val = DataLoader(hit_dataset_val, batch_size=batch_size, shuffle=True)\n",
        "for data in hit_loader_val:\n",
        "    print(data)\n",
        "    print(data.batch)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk-TbCPUch2b"
      },
      "source": [
        "We can now start with the implementation of the Particle-Net like GNN:\n",
        "\n",
        "Let's start with considering the features of the nodes we have to consider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9p7-FKncv57"
      },
      "outputs": [],
      "source": [
        "features_to_consider = 'PMTenergyS,PMTenergyC,PMTenergyComb,SiPMenergyS,SiPMenergyC,SiPMenergyComb,x,y,z,E'.split(',')\n",
        "\n",
        "\n",
        "class ParticleNetEdgeNet(nn.Module):\n",
        "    def __init__(self, in_size, layer_size):\n",
        "        super(ParticleNetEdgeNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(in_size*2, layer_size))           # directed Graph\n",
        "        #layers.append(nn.Linear(in_size*2, layer_size))           # un-directed Graph\n",
        "        layers.append(nn.BatchNorm1d(layer_size))\n",
        "        layers.append(nn.ReLU())\n",
        "\n",
        "        for i in range(2):\n",
        "            layers.append(nn.Linear(layer_size, layer_size))\n",
        "            layers.append(nn.BatchNorm1d(layer_size))\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{}(nn={})\".format(self.__class__.__name__, self.model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpkBfV-cdj_t"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6it-XnLdnFs"
      },
      "outputs": [],
      "source": [
        "class ParticleNet(nn.Module):\n",
        "    def __init__(self, node_feat_size, num_classes=1):\n",
        "        super(ParticleNet, self).__init__()\n",
        "        self.node_feat_size = node_feat_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.k = 10  #8                                            # K  number of connections to closest nodes in KNN cluster neighbor\n",
        "        self.num_edge_convs = 3                     # number of stacked EdgeConv blocks\n",
        "        self.kernel_sizes = [64, 128, 256]          # number of units in three linear transformation layers in EdgeConv blocks\n",
        "        #self.kernel_sizes = [32, 64, 128]          # number of units in three linear transformation layers in EdgeConv blocks\n",
        "        self.fc_size = 256                          # units  in the first fully-connected layer after EdgeConv blocks\n",
        "        self.dropout = 0.1\n",
        "        self.edge_nets = nn.ModuleList()\n",
        "        self.edge_convs = nn.ModuleList()\n",
        "\n",
        "        self.kernel_sizes.insert(0, self.node_feat_size)\n",
        "        self.output_sizes = np.cumsum(self.kernel_sizes)        # inputs also include previous layer output\n",
        "\n",
        "        # first EdgeConv block\n",
        "        self.edge_nets.append(ParticleNetEdgeNet(self.node_feat_size, self.kernel_sizes[1]))\n",
        "        self.edge_convs.append(EdgeConv(self.edge_nets[-1], aggr=\"mean\"))\n",
        "\n",
        "        # stack other EdgeConv blocks on the first\n",
        "        for i in range(1, self.num_edge_convs):\n",
        "            print(\"self.num_edge_convs\", i)\n",
        "            # add kernel sizes because of skip connections\n",
        "            self.edge_nets.append( ParticleNetEdgeNet(self.output_sizes[i], self.kernel_sizes[i+1]) )\n",
        "            self.edge_convs.append(EdgeConv(self.edge_nets[-1], aggr=\"mean\"))\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(self.output_sizes[-1], self.fc_size))\n",
        "        self.drouput_layer = nn.Dropout(p=self.dropout)\n",
        "        self.fc2 = nn.Linear(self.fc_size, self.num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        batch = data.batch\n",
        "        for i in range(self.num_edge_convs):\n",
        "            #print(x[:, 2:5])\n",
        "            # node features:  PMTenergyS, PMTenergyC, PMTenergyComb, SiPMenergyS, SiPMenergyC, SiPMenergyComb, hitX, hitY, hitZ, hitE'\n",
        "            edge_index = ( knn_graph(x[:, 6:9], self.k, batch) if i==0 else knn_graph(x, self.k, batch) )\n",
        "            #x = ( torch.cat(self.edge_convs[i](x, edge_index), x), dim=1 )\n",
        "            x = torch.cat( (self.edge_convs[i](x, edge_index), x), dim=1 )  # concatenating with original features i.e. skip connection\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrBEFPegd4nj"
      },
      "outputs": [],
      "source": [
        "def gnn_model_summary(model):\n",
        "    model_params_list = list(model.named_parameters())\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "    line_new = \"{:>20}  {:>25} {:>15}\".format(\"Layer.Parameter\", \"Param Tensor Shape\", \"Param #\")\n",
        "    print(line_new)\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "    for elem in model_params_list:\n",
        "        p_name = elem[0]\n",
        "        p_shape = list(elem[1].size())\n",
        "        p_count = torch.tensor(elem[1].size()).prod().item()\n",
        "        line_new = \"{:>20}  {:>25} {:>15}\".format(p_name, str(p_shape), str(p_count))\n",
        "        print(line_new)\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "    total_params = sum([param.nelement() for param in model.parameters()])\n",
        "    print(\"Total params:\", total_params)\n",
        "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"Trainable params:\", num_trainable_params)\n",
        "    print(\"Non-trainable params:\", total_params - num_trainable_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpLlzzKBfaNc"
      },
      "outputs": [],
      "source": [
        "def train_test_one_epoch(model,jets_loader, device, optimizer, loss_fn, batch_size, history, mode='train'):\n",
        "    if mode=='train':\n",
        "        model.train(True)\n",
        "    else:\n",
        "        model.train(False)\n",
        "\n",
        "    #epoch_loss = 0\n",
        "    #epoch_acc = 0\n",
        "    #return epoch_loss, epoch_acc\n",
        "    running_loss, running_correct = 0.,0.\n",
        "    tot_iter = len(jets_loader)\n",
        "    t = tqdm.tqdm(enumerate(jets_loader),total=tot_iter)\n",
        "    for i, data in t:\n",
        "        inputs = data # For the inputs we are passing the whole Data() object\n",
        "        labels = data.y.unsqueeze(1)\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        #print(data)\n",
        "        #print(type(data))\n",
        "\n",
        "        optimizer.zero_grad() # Clear gradients\n",
        "        #outputs = gnn(inputs) # Perform a single forward pass\n",
        "        outputs = model(inputs) # Perform a single forward pass\n",
        "\n",
        "        loss = loss_fn(outputs, labels) # Compute the loss\n",
        "        if mode=='train':\n",
        "            loss.backward()  # Derive gradients\n",
        "            optimizer.step() # Update parameters based on gradients.\n",
        "\n",
        "        running_loss += loss.item() #.item() returns average loss over the batch\n",
        "        #print(labels, outputs.T, loss)\n",
        "\n",
        "        del loss\n",
        "\n",
        "        #_, predicted = torch.max(F.softmax(outputs,dim=1).data, dim=1) # Adding softmax since there is none in the model, and taking the index of the highest probability.\n",
        "        #running_correct += float(torch.sum(predicted == labels.data))\n",
        "\n",
        "        #print(_)\n",
        "\n",
        "    epoch_loss = running_loss / tot_iter\n",
        "    #epoch_acc = running_correct / (tot_iter*batch_size)\n",
        "    history.append( running_loss/tot_iter)\n",
        "\n",
        "    return epoch_loss, outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qKf16NaGfgyM"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import tqdm\n",
        "\n",
        "\n",
        "gnn = ParticleNet(node_feat_size=len(features_to_consider), num_classes=1)\n",
        "gnn.to(device)\n",
        "#loss_fn = torch.nn.MSELoss()                            # Using Mean Squared Error as loss function for regression\n",
        "loss_fn = torch.nn.HuberLoss()                            # Using Mean Squared Error as loss function for regression\n",
        "\n",
        "optimizer = optim.Adam(gnn.parameters(), lr = 0.00075)\n",
        "EPOCHS = 20\n",
        "loss_train, loss_val = [],[]\n",
        "acc_train, acc_val = [],[]\n",
        "gnn_model_summary(gnn)\n",
        "print(hit_dataset_val[:10])\n",
        "\n",
        "history_train = []\n",
        "history_val = []\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "    loss_train_epoch,  y_pred_train = train_test_one_epoch(gnn, hit_loader_train, device, optimizer, loss_fn, batch_size, history_train, mode = 'train')\n",
        "    loss_val_epoch, y_pred_val = train_test_one_epoch(gnn, hit_loader_val, device, optimizer, loss_fn, batch_size, history_val, mode = 'test')\n",
        "    print('LOSS train {:.3f}, valid {:.3f}'.format(loss_train_epoch, loss_val_epoch))\n",
        "    loss_train.append(loss_train_epoch)\n",
        "    loss_val.append(loss_val_epoch)\n",
        "    #save model state with best parameters\n",
        "    torch.save(gnn.state_dict(), 'best-model-parameters.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4qjg3lpf5N3"
      },
      "outputs": [],
      "source": [
        "best_model_param = torch.load('best-model-parameters.pt')\n",
        "gnn.load_state_dict(best_model_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKl6k8WUgGzd"
      },
      "outputs": [],
      "source": [
        "pred_energy = []\n",
        "truth_energy = []\n",
        "pred_label = []\n",
        "true_label = []\n",
        "for i, data in enumerate(hit_loader_val):\n",
        "    inputs, truth = data, data.y\n",
        "    outData = data.x.cpu().detach().numpy()\n",
        "    y_true = data.y.cpu().detach().numpy()\n",
        "\n",
        "    inputs = inputs.to(device)\n",
        "    y_pred = gnn(inputs.to(device)).cpu().detach().numpy().squeeze(1)\n",
        "    truth_energy.append(y_true)\n",
        "    pred_energy.append(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ9TznCTgask"
      },
      "outputs": [],
      "source": [
        "def split_iterate(df, colname):\n",
        "    return [x for __, x in df.groupby(colname)]\n",
        "\n",
        "\n",
        "truth_energy = np.concatenate(truth_energy).ravel()\n",
        "pred_energy = np.concatenate(pred_energy).ravel()\n",
        "gnnRecoDF = pd.DataFrame({'TruthEnergy' : truth_energy, 'GNNenergy' : pred_energy})\n",
        "print(gnnRecoDF)\n",
        "results_array = split_iterate(gnnRecoDF, 'TruthEnergy')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sateY9P_g7d8"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "\n",
        "\n",
        "print(results_array[0])\n",
        "print(results_array[1])\n",
        "resolution = []\n",
        "fit_resolution = []\n",
        "fit_Epeak = []\n",
        "energies = []\n",
        "#energies = [e.TruthEnergy for e in results_array]\n",
        "for i in range(len(results_array)):\n",
        "    trueE = int(results_array[i].TruthEnergy.mean())\n",
        "    recoE = results_array[i].GNNenergy.mean()\n",
        "    recoE_std = results_array[i].GNNenergy.std()\n",
        "    resolution.append(recoE_std/trueE)\n",
        "    energies.append(trueE)\n",
        "    print(\"sigma/E: \", recoE_std/trueE)\n",
        "\n",
        "    bmin = trueE - 4*np.sqrt(trueE)\n",
        "    bmax = trueE + 4*np.sqrt(trueE)\n",
        "    myBins = np.linspace(bmin, bmax, 100)\n",
        "    #_, bins = plt.hist(myBins, bins=100, range=(bmin, bmax))\n",
        "    counts, bins = np.histogram(results_array[i].GNNenergy, bins=100, range=(bmin, bmax))\n",
        "    plt.hist(results_array[i].GNNenergy, bins=10, range=(bmin, bmax), density=1)\n",
        "\n",
        "    mu, sigma = scipy.stats.norm.fit(results_array[i].GNNenergy)\n",
        "    best_fit_line = scipy.stats.norm.pdf(bins, mu, sigma)\n",
        "    fit_resolution.append(sigma)\n",
        "    fit_Epeak.append(mu)\n",
        "\n",
        "    plt.plot(bins, best_fit_line)\n",
        "\n",
        "    #counts, bins = np.histogram(results_array[i].GNNenergy)\n",
        "    #plt.hist(bins[:-1], bins, weights=counts)\n",
        "    #plt.savefig('RecoE%d.png' %trueE)\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "\n",
        "\n",
        "plt.plot(energies, resolution)\n",
        "plt.xlabel('Energy [GeV]')\n",
        "plt.ylabel('sigma/E')\n",
        "plt.savefig('test.png')\n",
        "plt.clf()\n",
        "\n",
        "print(energies)\n",
        "energies = np.array(energies)\n",
        "x = 1/np.sqrt(energies)\n",
        "print(x)\n",
        "y = np.array(fit_resolution)/np.array(fit_Epeak)\n",
        "print(y)\n",
        "#plt.errorbar(x, y, yerr=np.sqrt(np.array(fit_resolution)/1000))\n",
        "coef = np.polyfit(x, y, 1)\n",
        "polyf = np.poly1d(coef)\n",
        "plt.plot(x, y, 'yo', x, polyf(x))\n",
        "print(\"Par0: \", coef[0], \"Par1: \", coef[1])\n",
        "print(\"Risoluzione: \", coef[0]*100, \"/sqrt(E) + \", coef[1]*100)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms2-rU_lmGU0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNczykJkTLB8VEVoDlj0dCK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}